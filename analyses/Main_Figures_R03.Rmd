---
title: "Assignment-Control Plots: A Visualization tool for Causal Inference Studies"
author: "Rachael C. Aikens, Michael Baiocchi"
output: 
  pdf_document:
    number_sections: true
bibliography: citations.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, warning = FALSE, message = FALSE, echo = FALSE, fig.align = "center")
library(RACplots)
library(optmatch)
library(DOS2)
library(tidyverse)
library(ggpubr)

theme_set(theme_light())

# set some universal variables
prop_model = formula(t ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10)
prog_model = formula(y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10)
k = 1
```

# Set-up

The results that follow depict several simulated datasets.  The primary generative model for these is based on @aikens2020pilot and is specified as follows:

\begin{align*}
X_i &\sim Normal(0, I_{10}) \\
T_i &\sim Bernoulli\left(\frac{1}{1 + exp(\phi(X_i))}\right) \\
Y_i(0) &= \Psi(X_i) + \epsilon_i \\
\epsilon_i &\sim N(0, 1) 
\end{align*}

where $\phi(X)$ and $\Psi(X)$ represent the true propensity and prognostic score functions, given by

\begin{align*}
\phi(X_i) &= c_1 X_{i1} - c_0 \\
\Psi(X_i) &= \rho X_{i1} + \sqrt{(1 - \rho^2)} X_{i2}.
\end{align*}

Where $c_1$, $c_0$, and $\rho$ are constants.  In particular, the form for the prognostic function above guarantees that $\rho = Corr(\phi(X), \Psi(X))$.

# Results

## Figure 1: Assignment-Control Plots and Data Diagnostics

A first use for the assignment-control plot is as an informal data diagnostic. Researchers running observational studies often want a basic understanding of the baseline variation between the treatment and control groups before they begin. Plotting the standardized mean differences between the treatment groups (love plots) is a common starting place to understand imbalances between groups, but when there are many covariates -- some of which are irrelevant -- it can be difficult to tell by eye which imbalances should be of most concern.  Histograms of propensity score overlap can be an important diagnostic for checking that the treatment and control groups overlap in terms of their probability of treatment (since this condition is essential for many causal inference methodologies).  However, the propensity score does not necessarily reflect all aspects of covariate balance which may be important. 

```{r fig.width=8, fig.height=5, fig.cap="Assignment-Control plots (A-C) and propensity score density histograms (D-F) for three simulated observational datasets. Red points represent treated observations, blue points represent control."}
rho <- 0.3
#simulate data
df_normal <- generate_data(N = 1500, p = 10, true_mu = "X1/3-3", rho = rho, sigma = 1)
df_prop_overlap_bad <- generate_data(N = 1500, p = 10, true_mu = "3*X1-6", rho = rho, sigma = 1)
df_prog_overlap_bad <- generate_data(N = 1500, p = 10, true_mu = "3*X1-6", rho = 0.9, sigma = 1)

a <- AC_plot(data = df_normal)
b <- AC_plot(data = df_prop_overlap_bad)
c <- AC_plot(data = df_prog_overlap_bad)

d <- propensity_histogram(data = df_normal)
e <- propensity_histogram(data = df_prop_overlap_bad)
f <- propensity_histogram(data = df_prog_overlap_bad)

ggarrange(a, b, c, d, e, f, ncol= 3, nrow = 2, heights = c(3, 2), labels = "AUTO")
```

```{r}
pdf("figures/Figure1.pdf",  width=8, height=5)
ggarrange(a, b, c, d, e, f, ncol= 3, nrow = 2, heights = c(3, 2), labels = "AUTO")
dev.off()
```

Figure 1 shows example assignment-control plots (A-C) and propensity score density histograms (D-F) for three simulated observational datasets. It is worth noting that the propensity score histogram for any dataset simply shows the marginal distribution of the data clouds shown in any assignment control plot. Panels A and D in Figure 1 depict the assignment-control plots and propensity histograms for a favorable observational dataset. There is substantial overlap between the treated and control subjects both in terms of their propensity for treatment and their likely outcome under the control assignment.  The correllation between treatment and prognosis is low.  A researcher viewing these diagnostic plots might be quite satisfied proceeding with their study.

The pairs of panels (B,E) and (C,F) depict diagnostic plots for a pair of unfavorable scenarios.  In (B,E), the overlap between treated and control individuals is much worse -- some individuals in the dataset have a much higher probability of treatment (close to 1) than others, indicating that the randomization assumption of strongly ignorable treatment assignment may be violated.  A researcher might be wary in such a situation, but might consider proceeding with a study, perhaps with some amount of trimming on the propensity score spectrum to improve propensity score overlap.  (C, F) depicts an even worse possibility: the problems of propensity score overlap are the same, but prognosis and treatment assignment are also highly correllated.  In a clinical study, this might mean that only the sickest patients are ever given the treatment. In a nutritional study, this might mean that only the patients with the most excellent baseline health sign up for a diet of program of interest.  This is an even more serious problem because the treated individuals are systematically very divergent from the control individuals in terms of baseline variation predictive of both the potential outcome and the treatment assginment.  A researcher in this situation must be very wary. Importantly, just looking at the propensity score histograms (E and F) does not allow a researcher to differentiate between these two cases.  Only by visualizing the joint distribution of propensity and prognosis does the issue of highly correllated treatment assignment and prognosis come to light.

## Figure 2: Assignment-Control Plots and Matching

```{r}
rho <- 0.5
#simulate data
df <- generate_data(N = 2000, p = 10, true_mu = "X1/3-3", rho = rho, sigma = 1)

# mahalanobis match
mahal_dist <- DOS2::smahal(df$t, df[,1:10])
m_match <- pairmatch(mahal_dist, controls = k, df)
```

```{r}
# Match on the true scores
oracle_prop_match <- pairmatch(t ~ prop, controls = k, df)

# mahalanobis match + caliper
mahal_caliper_dist <- addcaliper(mahal_dist, z = df$t, p = df$prop, caliper = 0.1)
mahal_jointcaliper_dist <- addcaliper(mahal_caliper_dist, z = df$t, p = df$prog, caliper = 0.1)
m_caliper_match <- pairmatch(mahal_caliper_dist, controls = k, df)
m_jointcaliper_match <- pairmatch(mahal_jointcaliper_dist, controls = k, df)
```

```{r, fig.width=5.5, fig.height= 5.5,fig.cap="Assignment control plots depicting four different 1-to-1 matching schemes.  Red points represent treated observations, blue points represent control.  Dotted lines connect matched individuals. (A) Mahalanobis distance matching, (B), propensity score matching, (C) Mahalanobis distance matching with a propensity score caliper, (D) Mahalanobis distance matching with propensity and prognostic score calipers."}
a <- AC_match_plot(df, m_match, title = "\nMahalanobis Match")
b <- AC_match_plot(df, oracle_prop_match, title = "\nPropensity Match")
c <- AC_match_plot(df, m_caliper_match, title = "Mahalanobis Match,\nPropensity Caliper")
d <- AC_match_plot(df, m_jointcaliper_match, title = "Mahalanobis Match,\nJoint Caliper")

ggarrange(a, b, c, d, ncol= 2, nrow = 2, labels = "AUTO" )
```


```{r}
pdf("figures/Figure2.pdf",  width=6, height=6)
ggarrange(a, b, c, d, ncol= 2, nrow = 2, labels = "AUTO" )
dev.off()
```

Assignment-Control Plots can also be a useful diagnostic tool for matching studies.  The four panels in figure 2 show the assignment-control visualizations of four different 1-to-1 matching schemes on the same data set: Mahalanobis distance (A), propensity score (B), Mahalanobis distance with a propensity score caliper (C), and Mahalanobis distance with both a propensity score caliper and a pronostic score caliper (D).  

In Mahalanobis distance matching, all covariates are weighted equally in a statistical sense.  When there is an abundance of uninformative covariates (i.e. those which are neither associated with the outcome nor the treatment assignment.), Mahalanobis distance matching can select matches that may actually be quite distant in the assignment-control space [@aikens2020pilot]. On the other hand, propensity score matching optimizes directly for matches which are nearby in terms of the variation associated with the treatment (the "assignment" axis), but it is entirely agnostic to variation associated with the outcome.  This can result in high variance in estimated treatment effect [@king2016propensity].  Finally, the two caliper methods impose contraints on the matching option to ensure that matches are close in terms of propensity score (C) or both propensity and prognostic score (D).  This visualization illustrates the potential of these methods for minimizing bias (stemming from poor propensity score balance) and variace (stemming from poor prognostic score balance) in the treatment effect.

## Figure 3: Randomization-Assignment-Control Plots

[comment]: <> (I really should introduce IVs somewhere before this so readers aren't blindsided by this)

@bhattacharya2007instrumental suggest that measured covariates that are valid instruments should *not* be included in propensity score models, since this may actually *increase* the bias and variance of the causal effect estimates in the absence of strong ignorability.  Intuitively, if some covariate captures some aspect of randomization or outcome-independent encouragement, we would actually like our treatment and control observations to be *far apart* in terms of the encouragement they recieve [@baiocchi2012near].  This supports the idea that subjects in an observational study might be best described by two separate quantities summarizing the baseline variation associated with the assignment mechanism: one "randomization" axis summarizing variation associated with the treatment assignment but unrelated to the outcome (i.e. a continuous instrumental variable or a combination of instruments), and one "assignment" axis summarizing variation associated with both the treatment assignment and the outcome (i.e. the propensity score).  This separation is important because observational study designs might do well to treat these two aspects of baseline variation differently. In particular, while matched subjects should ideally be very close in their propensity scores, they should ideally be very *distant* in terms of instrumental variation [@baiocchi2012near]. 
 
As an illustrative example, we consider the simulation set-up as before, except that now a new measured covariate $Z$, is present as an instrumental variable (IV):

\begin{align*}
    \phi(X_i, Z_i) &= c_1 X_{i1} + c_2 Z_{i} - c_0,\\
    \Psi(X_i) &=\rho X_{i1} + \sqrt{(1-\rho^2)}X_{i2}.
\end{align*}

Where $c_2$, $c_1$, $c_0$, and $\rho$ are constants.  And we take $Z_i$ to be

$$Z_i = \rho_z X_{i1} + \sqrt{1-\rho_z^2}\zeta_i$$

Where the $\zeta_i$ are iid standard normal and completely independent of all other covariates, and $\rho_z$ is a constant between 0 and 1.  In this setup, $\zeta$ might be thought of as the "pure" IV, while $Z$ represents an IV that - while still a valid instrumental variable - is only independent of the outcome given the treatment assignment *and* the observed covariates.  Importantly, while $Z$ is measured, the "pure" IV $\zeta$ is unmeasured.

```{r nearfar matching}
library(nearfar)
# Seeds
# Some patterning, but not beautiful: 123, 125
# Pretty good: 124
set.seed(124)
rho <- 0.5
IV_df <- generate_data_IV(N = 1000, p = 10, true_mu = "X1/2 + Z/2 - 3.25", rho = rho, rho_z = 0.2) %>%
  mutate(IV = zeta)

IVrange <- range(IV_df$Z)[2] - range(IV_df$Z)[1]


nf <- opt_nearfar(dta = IV_df, trt = "t", covs = c("X1", "X2"), iv = "Z",
                  trt.type = "bin", adjust.IV = TRUE, cutp.range = c(sd(IV_df$Z)*1.5, IVrange),
                  max.time.seconds = 300)
```

```{r}
reformat_nf <- function(nf, N){
  m_nf <- dim(nf$match[1])

  nf_reformatted <- nf$match %>%
    as_tibble() %>%
    mutate(match_id = paste("1.", 1:nrow(.), sep = "")) %>%
    gather("name", "index", -match_id) 

  match <- tibble(index = 1:N) %>%
    left_join(nf_reformatted) %>%
    pull(match_id)
  
  names(match) <- 1:N
  
  return(match)
}
```

```{r fig.height=6, fig.width=8}
IV_df$IV <- IV_df$zeta

nf_match <- reformat_nf(nf, 1000)

m <- sum(!is.na(nf_match))/2
subsample_mids <- paste("1.", sample(1:m, 30), sep = "")
nf_match_subsample <- ifelse(nf_match %in% subsample_mids, nf_match, NA)

a <- AC_filter_plot(data = IV_df, match = nf_match)
b <- RC_filter_plot(data = IV_df, match = nf_match)
c <- RA_filter_plot(data = IV_df, match = nf_match)

d <- AC_match_plot(data = IV_df, match = nf_match_subsample)
e <- RC_match_plot(data = IV_df, match = nf_match_subsample)
f <- RA_match_plot(data = IV_df, match = nf_match_subsample)

ggarrange(a, b, c, d, e, f, ncol = 3, nrow =2, labels = "AUTO")
```

```{r}
pdf("figures/Figure3.pdf",  width=8, height=6)
ggarrange(a, b, c, d, e, f, ncol = 3, nrow =2, labels = "AUTO")
dev.off()
```

## Figure 4: Assignment-Control Plots and Unmeasured Confounding

There is a wide and increasing variety of methods which use some articulation of propensity and prognosis to estimate a treatment effect. However, the theorems underlying the use of the propensity score and the theorems underlying the use of the prognostic score both depend on the absense of unmeasured confounding. Violations of this assumption have ramifications for a wide variety of causal inference approaches, and assignment-aontrol plots may be similarly misleading when unmeasured confounding is at play.

Figure 5 illustrates the behavior of assignment control plots in a scenario with unobserved confounding.  We add to our data-generating set-up an unobserved confounder, $U$, such that:

\begin{align*}
    \phi(X_i) &= c_1 X_{i1} -  \eta U - c_0,\\
    \Psi(X_i) &=\rho X_{i1} + \sqrt{(1-\rho^2)}X_{i2} + \eta U.
\end{align*}

Suppose we somehow ascertained exactly the correct relationships between the two score models and the *observed* covariates, so that our propensity and prognostic models are precisely $\hat{\phi}(X_i) = c_1X_{i1} - c_0$ and $\hat{\Psi}(X_i) = \rho X_{i1} + \sqrt{(1-\rho^2)}X_{i2}$, respectively.  That is, the score models are exactly correct, except that they do not include the unobserved confounder.  Figure 5 panels A-C depict the assignment control plots we might make and matchings we might produce using these score models, $\hat{\phi}$ and $\hat{\Psi}$.  Since both the assignment-control plots and the matchings use the score models $\hat{\phi}$ and $\hat{\Psi}$ that fail to capture unobserved confounding, our propensity matches appear quite close in $\hat{\phi}$ (Figure 5B) and our mahalanobis distance matchings with propensity and prognostic score calipers appear quite close in the assignment-control space defined by $\hat{\phi} \times \hat{\Psi}$ (Figure 5C).  

However, panels D-F in Figure 5 show the same matches in the *true* assignment-control space, in which $\phi$ and $\Psi$ are known to depend on the unobserved confounder, $U$.  In each matching, pairs tend to differ from each other due to baseline variations in the unobserved confounder which were not accounted for in the matching process.  The contrast between Figures 5C and 5F most cleanly illustrate how failing to account for $U$ results in systematic error: in the true assignment-control space, one matched individual in each pair tends to have both higher prognostic score and higher propensity score than its partner. Since this individual is more often the treated individual than the control individual, estimates of treatment effect based on this matching will tend to be biased, since $U$ induces systematic differences between paired individuals, even after matching.

```{r}
nu <- 0.3
rho <- 0.2
#simulate data
df <- generate_xSITA_data(N = 2000, p = 10, nu = nu, rho = rho, sigma = 1)

# mahalanobis match
mahal_dist <- match_on(prop_model, method = "mahalanobis", data = df)
m_match <- pairmatch(mahal_dist, controls = k, df)
```

```{r}
#Calculate best possible propensity and prognostic scores without knowing about confounder
naive_df <- df %>% 
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2, 
           prop = X1/3-3)

naive_prop_match <- pairmatch(t ~ prop, controls = k, naive_df)
mahal_caliper_dist <- addcaliper(mahal_dist, z = naive_df$t, p = naive_df$prop, caliper = 0.1)
mahal_caliper_dist <- addcaliper(mahal_caliper_dist, z = naive_df$t, p = naive_df$prog, caliper = 0.1)
m_caliper_match <- pairmatch(mahal_caliper_dist, controls = k, naive_df)
```

```{r,  fig.width=8, fig.height= 6, fig.cap="Assignment-control plots for three matching schemes on a dataset with unobserved confounding.  A-C depict the assignment-control space as ascertained without knowledge of the unobserved confounder.  D-F depict the true assignment-control space and the true match distances."}

a <- AC_match_plot(naive_df, m_match, title = "Mahalanobis Match")
b <- AC_match_plot(naive_df, naive_prop_match, title = "Propensity Match")
c <- AC_match_plot(naive_df, m_caliper_match, title = "Mahalanobis Match, Joint Calipers")

d <- AC_match_plot(df, m_match, title = "Mahalanobis Match")
e <- AC_match_plot(df, naive_prop_match, title = "Propensity Match")
f <- AC_match_plot(df, m_caliper_match, title = "Mahalanobis Match, Joint Calipers")

ggarrange(a,b,c,d, e, f,  ncol= 3, nrow = 2, labels = "AUTO" )
```

```{r}
pdf("figures/Figure4.pdf",  width=8, height=6)
ggarrange(a,b,c,d, e, f,  ncol= 3, nrow = 2, labels = "AUTO" )
dev.off()
```


## Figures 5-6: Applied example

See /analyses/CABG/CABG_women.Rmd

# References

---
title: "Assignment-Control Plots: A Visualization tool for Causal Inference Studies"
author: "Rachael C. Aikens, Michael Baiocchi"
output: 
  pdf_document:
    number_sections: true
bibliography: citations.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, warning = FALSE, message = FALSE, echo = FALSE, fig.align = "center")
library(RACplots)
library(optmatch)
library(DOS2)
library(tidyverse)
library(ggpubr)

theme_set(theme_light())

# set some universal variables
prop_model = formula(t ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10)
prog_model = formula(y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10)
k = 1
```

# Set-up

The results that follow depict several simulated datasets.  The primary generative model for these is based on @aikens2020pilot and is specified as follows:

\begin{align*}
X_i &\sim Normal(0, I_{10}) \\
T_i &\sim Bernoulli\left(\frac{1}{1 + exp(\phi(X_i))}\right) \\
Y_i(0) &= \Psi(X_i) + \epsilon_i \\
\epsilon_i &\sim N(0, 1) 
\end{align*}

where $\phi(X)$ and $\Psi(X)$ represent the true propensity and prognostic score functions, given by

\begin{align*}
\phi(X_i) &= c_1 X_{i1} - c_0 \\
\Psi(X_i) &= \rho X_{i1} + \sqrt{(1 - \rho^2)} X_{i2}.
\end{align*}

Where $c_1$, $c_0$, and $\rho$ are constants.  In particular, the form for the prognostic function above guarantees that $\rho = Corr(\phi(X), \Psi(X))$.

# Results

## Figure 1: Assignment-Control Plots and Data Diagnostics

A first use for the assignment-control plot is as an informal data diagnostic. Researchers running observational studies often want a basic understanding of the baseline variation between the treatment and control groups before they begin. Plotting the standardized mean differences between the treatment groups (love plots) is a common starting place to understand imbalances between groups, but when there are many covariates -- some of which are irrelevant -- it can be difficult to tell by eye which imbalances should be of most concern.  Histograms of propensity score overlap can be an important diagnostic for checking that the treatment and control groups overlap in terms of their probability of treatment (since this condition is essential for many causal inference methodologies).  However, the propensity score does not necessarily reflect all aspects of covariate balance which may be important. 

```{r fig.width=8, fig.height=5, fig.cap="Assignment-Control plots (A-C) and propensity score density histograms (D-F) for three simulated observational datasets. Red points represent treated observations, blue points represent control."}
set.seed(123)
#simulate data
df_0 <- generate_data(N = 1500, p = 10, true_mu = "3*X1-5", rho = 0, sigma = 1)
df_pos <- generate_data(N = 1500, p = 10, true_mu = "3*X1-5", rho = 0.9, sigma = 1)
df_neg <- generate_data(N = 1500, p = 10, true_mu = "3*X1-5", rho = -0.9, sigma = 1)

a <- AC_plot(data = df_0)
b <- AC_plot(data = df_pos)
c <- AC_plot(data = df_neg)

d <- propensity_histogram(data = df_0)
e <- propensity_histogram(data = df_pos)
f <- propensity_histogram(data = df_neg)

ggarrange(a, b, c, d, e, f, ncol= 3, nrow = 2, heights = c(3, 2), labels = "AUTO")
```

```{r}
pdf("figures/Figure1.pdf",  width=8, height=5)
ggarrange(a, b, c, d, e, f, ncol= 3, nrow = 2, heights = c(3, 2), labels = "AUTO")
dev.off()
```

Figure 1 shows example assignment-control plots (A-C) and propensity score density histograms (D-F) for three simulated observational data sets. Notably, the marginal distribution of the propensity score is identical across the three simulated scenarios, and the corresponding propensity score histograms (D-F) are qualitatively equivalent.  However, panels A-C reveal that the three settings are in fact quite different. In setting A, treated individuals are no different from control individuals in terms of their prognosis.  This means that even a naive comparison of treated and control subjects may give an unbiased treatment effect estimate under suitable conditions.  In settings B and C, treatment and control outcomes should not be directly compared, because the results would give a biased estimate of treatment effect.  Moreover, the direction of the correlation between propensity and prognosis suggests the direction of the bias from a naive comparison: If the individuals with the \textit{worst} prognosis are the \textit{least} likely to be treated (Figure 1B), we are more likely to \textit{overestimate} the effectiveness of the treatment in producing a more positive outcome.  If the individuals with the \textit{worst} prognosis are the \textit{most} likely to be treated (Figure 1C), we are liable to err in the \textit{opposite} direction.  Future work might consider whether this correlation indicates a tendency toward bias not only in naive comparisons, but for adjustment or subclassification approaches in which the score models are imperfect or matchings of treated and control individuals are not exact.

## Figure 2: Other Notable Scenarios

```{r}
n <- 1000
df_hamburger <- tibble(X1 = rnorm(n), X2 = rnorm(n), X3 = rbinom(n, 1, 0.8)) %>%
  mutate(prog = 3*X3 + X2/3,
         prop = X1 - 1) %>%
  mutate(t = as.factor(rbinom(n, 1, 1/(1 + exp(-prop)))))

#n <- 500
df_boomerang <- tibble(X1 = rnorm(n), X2 = rnorm(n)) %>%
  mutate(prog = 4*X1/5) %>%
  mutate(prop = -prog^2 -1 + X2/2) %>%
  mutate(t = as.factor(rbinom(n, 1, 1/(1 + exp(-prop)))))
```

```{r fig.width=5.33, fig.height=5}
a <- AC_plot(df_hamburger, opaque_class = "none")
b <- AC_plot(df_boomerang, opaque_class = "none")

c <- propensity_histogram(data = df_hamburger)
d <- propensity_histogram(data = df_boomerang)

ggarrange(a, b, ncol= 2, nrow = 1, labels = "AUTO")
```

```{r}
pdf("figures/Figure2.pdf",  width=5.33, height=3)
ggarrange(b, a, ncol= 2, nrow = 1, labels = "AUTO")
dev.off()
```

## Figure 3: Assignment-Control Plots and Matching

```{r}
rho <- 0.5
#simulate data
df <- generate_data(N = 2000, p = 10, true_mu = "X1/3-3", rho = rho, sigma = 1)

# mahalanobis match
mahal_dist <- DOS2::smahal(df$t, df[,1:10])
m_match <- pairmatch(mahal_dist, controls = k, df)
```

```{r}
# Match on the true scores
oracle_prop_match <- pairmatch(t ~ prop, controls = k, df)

# mahalanobis match + caliper
mahal_caliper_dist <- addcaliper(mahal_dist, z = df$t, p = df$prop, caliper = 0.1)
mahal_jointcaliper_dist <- addcaliper(mahal_caliper_dist, z = df$t, p = df$prog, caliper = 0.1)
m_caliper_match <- pairmatch(mahal_caliper_dist, controls = k, df)
m_jointcaliper_match <- pairmatch(mahal_jointcaliper_dist, controls = k, df)
```

```{r, fig.width=5.5, fig.height= 5.5,fig.cap="Assignment control plots depicting four different 1-to-1 matching schemes.  Red points represent treated observations, blue points represent control.  Dotted lines connect matched individuals. (A) Mahalanobis distance matching, (B), propensity score matching, (C) Mahalanobis distance matching with a propensity score caliper, (D) Mahalanobis distance matching with propensity and prognostic score calipers."}
a <- AC_match_plot(df, m_match, title = "\nMahalanobis Match")
b <- AC_match_plot(df, oracle_prop_match, title = "\nPropensity Match")
c <- AC_match_plot(df, m_caliper_match, title = "Mahalanobis Match,\nPropensity Caliper")
d <- AC_match_plot(df, m_jointcaliper_match, title = "Mahalanobis Match,\nJoint Caliper")

ggarrange(a, b, c, d, ncol= 2, nrow = 2, labels = "AUTO" )
```


```{r}
pdf("figures/Figure3.pdf",  width=6, height=6)
ggarrange(a, b, c, d, ncol= 2, nrow = 2, labels = "AUTO" )
dev.off()
```

Assignment-Control Plots can also be a useful diagnostic tool for matching studies.  The four panels in figure 2 show the assignment-control visualizations of four different 1-to-1 matching schemes on the same data set: Mahalanobis distance (A), propensity score (B), Mahalanobis distance with a propensity score caliper (C), and Mahalanobis distance with both a propensity score caliper and a pronostic score caliper (D).  

In Mahalanobis distance matching, all covariates are weighted equally in a statistical sense.  When there is an abundance of uninformative covariates (i.e. those which are neither associated with the outcome nor the treatment assignment.), Mahalanobis distance matching can select matches that may actually be quite distant in the assignment-control space [@aikens2020pilot]. On the other hand, propensity score matching optimizes directly for matches which are nearby in terms of the variation associated with the treatment (the "assignment" axis), but it is entirely agnostic to variation associated with the outcome.  This can result in high variance in estimated treatment effect [@king2016propensity].  Finally, the two caliper methods impose contraints on the matching option to ensure that matches are close in terms of propensity score (C) or both propensity and prognostic score (D).  This visualization illustrates the potential of these methods for minimizing bias (stemming from poor propensity score balance) and variace (stemming from poor prognostic score balance) in the treatment effect.

```{r}
set.seed(123)

rho <- 0.85

df_overlap_poor <- generate_data(N = 1000, p = 10, true_mu = "4*X1/3 - 2", rho = rho) 

prop_match <- pairmatch(t ~ prop, controls = 1, df_overlap_poor )
m_prop <- sum(!is.na(prop_match))/2
subsample_mids_prop <- paste("1.", sample(1:m_prop, 40), sep = "")
prop_match_subsample <- ifelse(prop_match %in% subsample_mids_prop, prop_match, NA)


b <- AC_match_plot(data = df_overlap_poor, match = prop_match_subsample, title = "Propensity Score Matching,\nPoor Overlap")

rho <- 0.85

df_overlap_good <- generate_data(N = 1000, p = 10, true_mu = "2*X1/3 - 2", rho = rho) 

prop_match <- pairmatch(t ~ prop, controls = 1, df_overlap_good )
m_prop <- sum(!is.na(prop_match))/2
subsample_mids_prop <- paste("1.", sample(1:m_prop, 40), sep = "")
prop_match_subsample <- ifelse(prop_match %in% subsample_mids_prop, prop_match, NA)


a <- AC_match_plot(data = df_overlap_good, match = prop_match_subsample, title = "Propensity Score Matching,\nGood Overlap")
```


```{r fig.width=5.33, fig.height=3}
ggarrange(a, b, ncol = 2, nrow = 1)
```

```{r}
pdf("figures/Figure4.pdf",  width=6, height=3)
ggarrange(a, b, ncol= 2, nrow = 1, labels = "AUTO" )
dev.off()
```
## Figure 5: Assignment-Control Plots and Unmeasured Confounding

There is a wide and increasing variety of methods which use some articulation of propensity and prognosis to estimate a treatment effect. However, the theorems underlying the use of the propensity score and the theorems underlying the use of the prognostic score both depend on the absense of unmeasured confounding. Violations of this assumption have ramifications for a wide variety of causal inference approaches, and assignment-aontrol plots may be similarly misleading when unmeasured confounding is at play.

Figure 5 illustrates the behavior of assignment control plots in a scenario with unobserved confounding.  We add to our data-generating set-up an unobserved confounder, $U$, such that:

\begin{align*}
    \phi(X_i) &= c_1 X_{i1} +  \eta U - c_0,\\
    \Psi(X_i) &=\rho X_{i1} + \sqrt{(1-\rho^2)}X_{i2} + \eta U.
\end{align*}

Suppose we somehow ascertained exactly the correct relationships between the two score models and the *observed* covariates, so that our propensity and prognostic models are precisely $\hat{\phi}(X_i) = c_1X_{i1} - c_0$ and $\hat{\Psi}(X_i) = \rho X_{i1} + \sqrt{(1-\rho^2)}X_{i2}$, respectively.  That is, the score models are exactly correct, except that they do not include the unobserved confounder.  Figure 5 panels A-C depict the assignment control plots we might make and matchings we might produce using these score models, $\hat{\phi}$ and $\hat{\Psi}$.  Since both the assignment-control plots and the matchings use the score models $\hat{\phi}$ and $\hat{\Psi}$ that fail to capture unobserved confounding, our propensity matches appear quite close in $\hat{\phi}$ (Figure 5B) and our mahalanobis distance matchings with propensity and prognostic score calipers appear quite close in the assignment-control space defined by $\hat{\phi} \times \hat{\Psi}$ (Figure 5C).  

However, panels D-F in Figure 5 show the same matches in the *true* assignment-control space, in which $\phi$ and $\Psi$ are known to depend on the unobserved confounder, $U$.  In each matching, pairs tend to differ from each other due to baseline variations in the unobserved confounder which were not accounted for in the matching process.  The contrast between Figures 5C and 5F most cleanly illustrate how failing to account for $U$ results in systematic error: in the true assignment-control space, one matched individual in each pair tends to have both higher prognostic score and higher propensity score than its partner. Since this individual is more often the treated individual than the control individual, estimates of treatment effect based on this matching will tend to be biased, since $U$ induces systematic differences between paired individuals, even after matching.

```{r}
nu <- 0.3
rho <- 0.2
#simulate data
df <- generate_xSITA_data(N = 2000, p = 10, nu = nu, rho = rho, sigma = 1)

# mahalanobis match
mahal_dist <- match_on(prop_model, method = "mahalanobis", data = df)
m_match <- pairmatch(mahal_dist, controls = k, df)
```

```{r}
#Calculate best possible propensity and prognostic scores without knowing about confounder
naive_df <- df %>% 
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2, 
           prop = X1/3-3)

naive_prop_match <- pairmatch(t ~ prop, controls = k, naive_df)
mahal_caliper_dist <- addcaliper(mahal_dist, z = naive_df$t, p = naive_df$prop, caliper = 0.1)
mahal_caliper_dist <- addcaliper(mahal_caliper_dist, z = naive_df$t, p = naive_df$prog, caliper = 0.1)
m_caliper_match <- pairmatch(mahal_caliper_dist, controls = k, naive_df)
```

```{r,  fig.width=8, fig.height= 6, fig.cap="Assignment-control plots for three matching schemes on a dataset with unobserved confounding.  A-C depict the assignment-control space as ascertained without knowledge of the unobserved confounder.  D-F depict the true assignment-control space and the true match distances."}

a <- AC_match_plot(naive_df, m_match, title = "Mahalanobis Match")
b <- AC_match_plot(naive_df, naive_prop_match, title = "Propensity Match")
c <- AC_match_plot(naive_df, m_caliper_match, title = "Mahalanobis Match, Joint Calipers")

d <- AC_match_plot(df, m_match, title = "Mahalanobis Match")
e <- AC_match_plot(df, naive_prop_match, title = "Propensity Match")
f <- AC_match_plot(df, m_caliper_match, title = "Mahalanobis Match, Joint Calipers")

ggarrange(a,b,c,d, e, f,  ncol= 3, nrow = 2, labels = "AUTO" )
```

```{r}
pdf("figures/Figure5.pdf",  width=8, height=6)
ggarrange(a,b,c,d, e, f,  ncol= 3, nrow = 2, labels = "AUTO" )
dev.off()
```


## Figure 6: Randomization-Assignment-Control Plots

@bhattacharya2007instrumental suggest that measured covariates that are valid instruments should *not* be included in propensity score models, since this may actually *increase* the bias and variance of the causal effect estimates in the absence of strong ignorability.  Intuitively, if some covariate captures some aspect of randomization or outcome-independent encouragement, we would actually like our treatment and control observations to be *far apart* in terms of the encouragement they recieve [@baiocchi2012near].  This supports the idea that subjects in an observational study might be best described by two separate quantities summarizing the baseline variation associated with the assignment mechanism: one "randomization" axis summarizing variation associated with the treatment assignment but unrelated to the outcome (i.e. a continuous instrumental variable or a combination of instruments), and one "assignment" axis summarizing variation associated with both the treatment assignment and the outcome (i.e. the propensity score).  This separation is important because observational study designs might do well to treat these two aspects of baseline variation differently. In particular, while matched subjects should ideally be very close in their propensity scores, they should ideally be very *distant* in terms of instrumental variation [@baiocchi2012near]. 
 
As an illustrative example, we consider the simulation set-up as before, except that now a new measured covariate $Z$, is present as an instrumental variable (IV):

\begin{align*}
    \phi(X_i, Z_i) &= c_1 X_{i1} + c_2 Z_{i} - c_0,\\
    \Psi(X_i) &=\rho X_{i1} + \sqrt{(1-\rho^2)}X_{i2}.
\end{align*}

Where $c_2$, $c_1$, $c_0$, and $\rho$ are constants.  And we take $Z_i$ to be

$$Z_i = \rho_z X_{i1} + \sqrt{1-\rho_z^2}\zeta_i$$

Where the $\zeta_i$ are iid standard normal and completely independent of all other covariates, and $\rho_z$ is a constant between 0 and 1.  In this setup, $\zeta$ might be thought of as the "pure" IV, while $Z$ represents an IV that - while still a valid instrumental variable - is only independent of the outcome given the treatment assignment *and* the observed covariates.  Importantly, while $Z$ is measured, the "pure" IV $\zeta$ is unmeasured. 

```{r nearfar matching}
library(nearfar)
# seeds
# 124 okay
set.seed(125)
rho <- 0.5
IV_df <- generate_data_IV(N = 1000, p = 10, true_mu = "X1/2 + 2*Z - 1.75", rho = rho, rho_z = 0.2) %>%
  mutate(IV = zeta)

AC_plot(IV_df)

IVrange <- range(IV_df$Z)[2] - range(IV_df$Z)[1]

nf <- opt_nearfar(dta = IV_df, trt = "t", covs = c("X1", "X2"), iv = "Z",
                  trt.type = "bin", adjust.IV = TRUE, cutp.range = c(sd(IV_df$Z)*1.5, IVrange),
                  max.time.seconds = 600)
```

```{r}
reformat_nf <- function(nf, N){
  m_nf <- dim(nf$match[1])

  nf_reformatted <- nf$match %>%
    as_tibble() %>%
    mutate(match_id = paste("1.", 1:nrow(.), sep = "")) %>%
    gather("name", "index", -match_id) 

  match <- tibble(index = 1:N) %>%
    left_join(nf_reformatted) %>%
    pull(match_id)
  
  names(match) <- 1:N
  
  return(match)
}
```

```{r fig.height=6, fig.width=8}
IV_df$IV <- IV_df$zeta

nf_match <- reformat_nf(nf, 1000)

m <- sum(!is.na(nf_match))/2
subsample_mids <- paste("1.", sample(1:m, 30), sep = "")
nf_match_subsample <- ifelse(nf_match %in% subsample_mids, nf_match, NA)

a <- AC_filter_plot(data = IV_df, match = nf_match)
b <- CR_filter_plot(data = IV_df, match = nf_match)
c <- AR_filter_plot(data = IV_df, match = nf_match)

d <- AC_match_plot(data = IV_df, match = nf_match_subsample)
e <- CR_match_plot(data = IV_df, match = nf_match_subsample)
f <- AR_match_plot(data = IV_df, match = nf_match_subsample)

ggarrange(a, b, c, d, e, f, ncol = 3, nrow =2, labels = "AUTO")
```

Time to consider something different for this visualization.

```{r}
prop_match <- pairmatch(t ~ prop, controls = 1, IV_df)
m_prop <- sum(!is.na(prop_match))/2
subsample_mids_prop <- paste("1.", sample(1:m_prop, 30), sep = "")
prop_match_subsample <- ifelse(prop_match %in% subsample_mids_prop, prop_match, NA)

# mahalanobis match
mahal_dist <- match_on(formula(t ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10),
                       method = "mahalanobis", data = IV_df)
m_match <- pairmatch(mahal_dist, controls = 1, IV_df)
m_mahal <- sum(!is.na(m_match))/2
subsample_mids_mahal <- paste("1.", sample(1:m_mahal, 30), sep = "")
mahal_match_subsample <- ifelse(m_match %in% subsample_mids_mahal, m_match, NA)

```

```{r fig.width=8, fig.height=6}
a <- AC_match_plot(data = IV_df, match = prop_match_subsample)
b <- CR_match_plot(data = IV_df, match = prop_match_subsample)
c <- AR_match_plot(data = IV_df, match = prop_match_subsample)

d <- AC_match_plot(data = IV_df, match = nf_match_subsample)
e <- CR_match_plot(data = IV_df, match = nf_match_subsample)
f <- AR_match_plot(data = IV_df, match = nf_match_subsample)

top_row <- ggarrange(a, b, c, ncol = 3, nrow = 1) %>% 
  annotate_figure(top = text_grob("Propensity Score Match, Randomization-Control-Assignment-plots", size = 14))

bottom_row <- ggarrange(d, e, f, ncol = 3, nrow = 1) %>% 
  annotate_figure(top = text_grob("Nearfar Match, Randomization-Control-Assignment-plots", size = 14))

ggarrange(top_row, bottom_row, ncol = 1, nrow = 2, labels = "AUTO")
```

```{r}
pdf("figures/Figure6.pdf",  width=8, height=6)
ggarrange(top_row, bottom_row, ncol = 1, nrow = 2, labels = "AUTO")
dev.off()
```

## Figures 7-8: Applied example

See /analyses/CABG/CABG_women.Rmd

# References

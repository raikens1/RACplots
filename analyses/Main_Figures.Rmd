---
title: "Assignment-Control Plots: A Visualization tool for Causal Inference Studies"
author: "Rachael Caelie (Rocky) Aikens"
output: 
  pdf_document:
    number_sections: true
bibliography: citations.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, warning = FALSE, message = FALSE, echo = FALSE, fig.align = "center")
library(RACplots)
library(optmatch)
library(DOS2)
library(dplyr)
library(ggplot2)
library(ggpubr)

theme_set(theme_light())
```

# Abstract

# Introduction

A fundamental problem of causal inference is the impossibility of observing counterfactuals: Once an individual has recieved a treatment or exposure, we can never observe what might have happened to that individual had they not recieved treatment, and vice versa.  Most studies of causal inference attempt to address this problem by comparing samples of treated individuals with samples of untreated individuals, ideally in some setting which controls for ways that these two groups might systematically differ.  Intuitively, one can imagine that our observations of the control sample are used as approximations in order to understand what *might* have happened to the treated individuals had they been untreated.  A question fundamental to these approaches is: How would we like our compared treated and control samples to be similar in order to obtain a clear understanding of the causal effect?  Said another way, what aspects of baseline variation are most important to address estimate the causal effect with minimal bias and variance? In the randomized experimental setting, we would like our compared groups to be balanced in terms of variation important to the outcome, since this reduces the variance of our estimates.  In the observational setting, we have the additional burden of correcting for bias-inducing imbalances stemming from an unknown assignment mechanism.

Researchers have proposed a variety of matching and conditioning methods to address baseline variation between treated and control samples variety of observational contexts. One popular approach is subclassification or adjustment on an estimated propensity score, which summarizes the measured baseline variation influencing the probability of assignment. Intuitively, propensity score methods attempt to model the treatment assignment mechanism based on observed covariates, so that it can be adjusted for. Under suitable assumptions, matching exactly on the propensity score recapitulates a completely randomized controlled experiment, allowing for identification and unbiased estimation of treatment effect.  However, conditioning solely on the propensity score may neglect baseline variation which is unimportant for treatment assignment but influential on the _potential outcomes_, potentially resulting unfavorably high variance and low power. For this reason, propensity score matching has been critiqued as statistically inefficient compared to other methods which optimize for a more comprehensive form of covariate similarity between matched sets [@king2016propensity]. 

In light of these critiques, methods such as Mahalanobis distance matching - which seek to establish similarity between matched sets over _all_ measured covariates - seem appealing. However, not all measured covariates are necessarily important to the causal problem - especially as researchers collect more and more comprehensive observational data with many measured covariates. The less-commmonly known prognostic score, formalized by @hansen2008prognostic, models the expected outcome of each subject under the control assignment, based on the observed covariates.  Matching or conditioning on the prognostic score reflects the ideal of controlling for baseline variation influencing the potential outcomes under the control assignment. Interestingly, under suitable assumptions, balancing on the prognostic score results in a form of covariate balance which leads to unbaised estimation of the causal effect, analogous to the propensity score.  The primary difference is that the propensity score controls for baseline variation influencing treatment assignment, whereas the prognostic score controls for baseline variation influencing the potential outcome under the control assignment. A small but growing body of literature suggests that methods which match jointly on a prognostic score and a propensity score may be a favorable approach in some observational contexts, optimizing directly for propensity score balance (which reduces bias), and for prognostic score balance (which reduces bias as well as variance and increases power in analyses of gamma sensitivity)[@leacy2014joint;@antonelli2018doubly;@aikens2020pilot].

Estimating the prognostic score in a way that does not overfit is a somewhat nuanced problem discussed in detail by @aikens2020pilot and summarized in section 3.3.  However, once a prognostic model is fit, it is possible to generate assignment control plots (indroduced in @aikens2020pilot), which visualize each subject in an observational dataset according to their propensity and prognostic scores.  These are two (often interrelated) features which are directly relevant to observational studies of causality: propensity score similarity between compared individuals reduces bias, while prognostic score similarity between compared individuals reduces bias as well as variance and increases power in sensitivity analyses for unobserved confounding. While assignment-control plots are a relatively minor point in this original study, here we will describe a more detailed investigation of the uses of assignment-control plots in visualization, as well possible extensions to instrumental variable studies and regression discontinuity.  We suggest that assigment control plots and variations thereof may be a useful visualization and teaching tool in many branches of causal inference research.

# Methods

## Notation and Background

We adopt the Neyman-Rubin potential outcomes framework, in which a sample is described by 

$$\mathcal{D} = \{(X_i, T_i, Y_i)\}_{i = 1}^n$$
where the triplet $(X_i, T_i, Y_i)$ describes an individual with measured covariates $X_i$, binary treatment assignment indicator $T_i$, and observed outcome $Y_i$.  We take $Y_i(T)$ to represent the potential outcome of individual $i$ under treatment assignment $T$. The fundamental problem of causal inference is that it is impossible to observe both potential outcomes, $Y_i(0)$ and $Y_i(1)$ for any individual.

The propensity score is defined as $e(X) = P(T=1|X)$.  The popularity of the propensity score in observational studies stems primarily from its use as a balancing score, i.e.

$$T \bot X | e(X)$$.

That is, within level-sets of the propensity score, the treatment assignment is independent of the measured covariates. Under the assumption of strongly ignorable treatment assignment, exact matching on the propensity score allows for unbiased estimation of the treatment effect [@rosenbaum1983central].

The prognostic score is defined by Hansen as any quantity $\Psi(X)$ such that 

$$Y(0) \bot X | \Psi(X)$$
Inessense, a prognostic score is any function of the measured covariates which -- through conditioning -- induces independence between the potential outcome under the control assignment and the measured covariates.  It is thus, by definition, a balancing score as well.  Under regularity conditions analogous to those for the propensity score, conditioning on the prognostic score also allows for unbiased estimation of the treatment effect, as described in more detail by @hansen2008prognostic.  When $Y(0) | X$ follows a generalized linear model $\Psi(X) = E[Y(0)|X]$.  In the literature, the prognostic score is often treated more informally as a model for the expected outcome under the control assignment given the observed covariates.

## Set-up

The results that follow depict several simulated datasets.  The primary generative model for these is based on @aikens2020pilot and is specified as follows:

\begin{align*}
X_i &\sim Normal(0, I_{10}) \\
T_i &\sim Bernoulli\left(\frac{1}{1 + exp(\phi(X_i))}\right) \\
Y_i(0) &= \tau \Psi(X_i) + \epsilon_i \\
\epsilon_i &\sim N(0, 1) 
\end{align*}

where $\phi(X)$ and $\Psi(X)$ represent the true propensity and prognostic score functions, given by

\begin{align*}
\phi(X_i) &= c_1 X_{i1} - c_0 \\
\Psi(X_i) &= \rho X_{i1} + \sqrt{(1 - \rho^2) X_{i2}}.
\end{align*}

Where $c_1$, $c_0$, and $\rho$ are constants.  In particular, the form for the prognostic function above guarantees that $\rho = Corr(\phi(X), \Psi(X))$.

## Fitting the Score Models

In real observational studies, the propensity and prognostic score models are not known.  Conventionally, the propensity scores are often estimated from a logistic regression of the baseline covariates on treatment assignment, fit on the entire study sample.  Fitting the prognostic score may be somewhat more nuanced (@aikens2020pilot).  First, since the prognostic model is meant to predict the outcome under the control assignment, the prognostic model is fit only on controls.  Thus, all prognostic score estimates on the treatment population are necessarily extrapolations.  Second, fitting the prognostic model on the entire control population raises concerns of overfitting [@hansen2008prognostic;@abadie2018endogenous].  In order to avoid concerns of overfitting and preserve the separation of the design and analysis phases of the study, @aikens2020pilot, propose a Pilot Design, in which a subset of the control individuals is selected and held aside for the purpose of fitting the prognostic model.  These controls - comprising a "Pilot data set" are then discarded, so that the observational units used to train the prognostic model are dijoint from the set used in the final analysis.  The question of how to optimally select the control observations for the pilot set is a difficult one, described in more detail elsewhere [@aikens2020pilot; @aikens2020stratified].

In order to clearly demonstrate the ideas behind the assignment control plots in this paper, most of the examples that follow bypass the problem of score estimation by using the ground-truth propensity and prognostic scores, as specified in our simulations.  The final subsection of the results discusses in more detail the realities of imprecision in the score estimates.

# Results

```{r utility functions}
match_viz <- function(data, match, rho, k = 1, title = "Matching"){
  plt_data <- data %>% 
    mutate(m = match) %>%
    mutate(a = ifelse (is.na(m), 0.9, 1)) %>% 
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2, 
           prop = mu,
           t = as.factor(abs(1-t))) %>%
    dplyr::select(c(t, prog, prop, m, a))
  
  m_data <- plt_data %>% 
    filter(!is.na(m)) %>%
    arrange(m, desc(t)) %>% 
    mutate(id = rep(1:(k + 1), sum(data$t))) %>%
    dplyr::select(-c(t, a)) %>%
    group_by(m) %>%
    summarize(prop1 = first(prop), prop2 = last(prop),
              prog1 = first(prog), prog2 = last(prog)) %>%
    dplyr::select(prog1, prog2, prop1, prop2)
  
  plt <- ggplot(data = plt_data, aes( x = prop, y = prog, group = t, color = t)) + 
    geom_point(aes(alpha = a), size = 1)+
    scale_color_brewer(palette="Set1") +
    geom_segment(data = m_data, 
                 aes(x = prop1, y = prog1,
                     xend = prop2, yend = prog2),
                 color =  "black", group = NA, linetype = "dashed") +
    ggtitle( title)+
    theme(legend.position = "none", aspect.ratio=1, plot.title = element_text(hjust = 0.5, size = 9))+
    ylab(expression(paste("Prognosis, ", Psi, "(x)", sep = ""))) +
    xlab(expression(paste("Propensity, ", phi, "(x)", sep = "")))
  
  return(plt)
}

overlap_histogram <- function(data){
    plt_data <- data %>% 
      mutate(prog = rho*X1 + sqrt(1-rho^2)*X2, 
           prop = mu,
           t = as.factor(abs(1-t))) %>%
      dplyr::select(c(t, prog, prop))
    
    ggplot(plt_data, aes(x = prop, y=..density.., fill = t)) + 
      geom_histogram(alpha = 0.4, position = "identity") +
      scale_fill_brewer(palette = "Set1") +
      theme(legend.position = "none") +
      xlab(expression(paste("Propensity, ", phi, "(x)", sep = ""))) +
      ylab("Density")
}

AC_plot <- function(data, rho, title = ""){
  plt_data <- data %>%
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2, 
           prop = mu,
           t = as.factor(abs(1-t)),
           a = ifelse(t == 1, 0.9, 1)) %>%
    dplyr::select(c(t, prog, prop, a))
  
  plt <- ggplot(data = plt_data, aes( x = prop, y = prog, group = t, color = t)) + 
    geom_point(size = 1, aes(alpha = a)) +
    scale_color_brewer(palette="Set1") +
    ggtitle(title) +
    theme(legend.position = "none", aspect.ratio=1, plot.title = element_text(hjust = 0.5, size = 9))+
    ylab(expression(paste("Prognosis, ", Psi, "(x)", sep = ""))) +
    xlab(expression(paste("Propensity, ", phi, "(x)", sep = "")))
  
  return(plt)
}
```


## Assignment-Control Plots and Data Diagnostics

```{r fig.width=8, fig.height=5, fig.cap="Assignment-Control plots (A-C) and propensity score density histograms (D-F) for three simulated observational datasets"}
set.seed(125)
rho <- 0.3
#simulate data
df_normal <- generate_data(N = 1500, p = 10, true_mu = "X1/3-3", rho = rho, sigma = 1)
df_prop_overlap_bad <- generate_data(N = 1500, p = 10, true_mu = "3*X1-6", rho = rho, sigma = 1)
df_prog_overlap_bad <- generate_data(N = 1500, p = 10, true_mu = "3*X1-6", rho = 0.9, sigma = 1)

a <- AC_plot(data = df_normal, rho)
b <- AC_plot(data = df_prop_overlap_bad, rho)
c <- AC_plot(data = df_prog_overlap_bad, 0.9)

d <- overlap_histogram(data = df_normal)
e <- overlap_histogram(data = df_prop_overlap_bad)
f <- overlap_histogram(data = df_prog_overlap_bad)

ggarrange(a, b, c, d, e, f, ncol= 3, nrow = 2, heights = c(3, 2), labels = "AUTO")
```


A first use for the assignment-control plot is as an informal data diagnostic. Researchers running observational studies often want a basic understanding of the baseline variation between the treatment and control groups before they begin. Plotting the standardized mean differences between the treatment groups is a common starting place to understand imbalances between groups, but when there are many covariates -- some of which are irrelevant -- it can be difficult to tell by eye which imbalances should be of most concern.  Histograms of propensity score overlap can be an important diagnostic for checking that the treatment and control groups overlap in terms of their probability of treatment (since this condition is essential for many causal inference methodologies).  However, the propensity score does not necessarily reflect all aspects of covariate balance which may be important. 

Figure 1 shows example assignment-control plots (A-C) and propensity score density histograms (D-F) for three simulated observational datasets. It is worth noting that the propensity score histogram for any dataset simply shows the marginal distribution of the data clouds shown in any assignment control plot. Panels A and D in Figure 1 depict the assignment-control plots and propensity histograms for a favorable observational dataset. There is substantial overlap between the treated and control subjects both in terms of their propensity for treatment and their likely outcome under the control assignment.  The correllation between treatment and prognosis is low.  A researcher viewing these diagnostic plots might be quite satisfied proceeding with their study.

The pairs of panels (B,E) and (C,F) depict diagnostic plots for a pair of unfavorable scenarios.  In (B,E), the overlap between treated and control individuals is much worse -- some individuals in the dataset have a much higher probability of treatment (close to 1) than others, indicating that the randomization assumption of strongly ignorable treatment assignment may be violated.  A researcher might be wary in such a situation, but might consider proceeding with a study, perhaps with some amount of trimming on the propensity score spectrum to improve propensity score overlap.  (C, F) depicts an even worse possibility: the problems of propensity score overlap are the same, but prognosis and treatment assignment are also highly correllated.  In a clinical study, this might mean that only the sickest patients are ever given the treatment. In a nutritional study, this might mean that only the patients with the most excellent baseline health sign up for a diet of program of interest.  This is an even more serious problem because the treated individuals are systematically very divergent from the control individuals in terms of baseline variation predictive of both the potential outcome and the treatment assginment.  A researcher in this situation must be very wary. Importantly, just looking at the propensity score histograms (E and F) does not allow a researcher to differentiate between these two cases.  Only by visualizing the joint distribution of propensity and prognosis does the issue of highly correllated treatment assignment and prognosis come to light.

## Assignment-Control Plots and Matching

Assignment-Control Plots can also be a useful diagnostic tool for matching studies.  The four panels in figure 2 show the assignment-control visualizations of four different 1-to-1 matching schemes on the same data set: Mahalanobis distance (A), propensity score (B), Mahalanobis distance with a propensity score caliper (C), and Mahalanobis distance with both a propensity score caliper and a pronostic score caliper (D).  

```{r}
rho <- 0.5
#simulate data
df <- generate_data(N = 2000, p = 10, true_mu = "X1/3-3", rho = rho, sigma = 1)
k = 1
prop_model = formula(t ~ . - mu - y)

# mahalanobis match
mahal_dist <- smahal(df$t, df[,1:10])
m_match <- pairmatch(mahal_dist, controls = k, df)
```

```{r}
#Calculate true propensity and prognostic score, and match on the true score
oracle_df <- df %>% 
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2, 
           prop = 1/(1+exp(-(mu))))

oracle_prop_match <- pairmatch(t ~ prop, controls = k, oracle_df)

# mahalanobis match + caliper
mahal_caliper_dist <- addcaliper(mahal_dist, z = oracle_df$t, p = oracle_df$prop, caliper = 0.1)
mahal_jointcaliper_dist <- addcaliper(mahal_caliper_dist, z = df$t, p = oracle_df$prog, caliper = 0.1)
m_caliper_match <- pairmatch(mahal_caliper_dist, controls = k, df)
m_jointcaliper_match <- pairmatch(mahal_jointcaliper_dist, controls = k, df)
```

```{r, fig.width=5.5, fig.height= 5.5,fig.cap="Assignment control plots depicting three different 1-to-1 matching schemes.  (A) Mahalanobis distance matching, (B), propensity score matching, (C) Mahalanobis distance matching with a propensity score caliper."}
a <- match_viz(df, m_match, rho, title = "\nMahalanobis Match")
b <- match_viz(df, oracle_prop_match, rho, title = "\nPropensity Match")
c <- match_viz(df, m_caliper_match, rho, title = "Mahalanobis Match,\nPropensity Caliper")
d <- match_viz(df, m_jointcaliper_match, rho, title = "Mahalanobis Match,\nJoint Caliper")

ggarrange(a, b, c, d, ncol= 2, nrow = 2, labels = "AUTO" )
```

In Mahalanobis distance matching, all covariates are weighted equally in a statistical sense.  When there is an abundance of uninformative covariates (i.e. those which are neither associated with the outcome nor the treatment assignment.), Mahalanobis distance matching can select matches that may actually be quite distant in the assignment-control space [@aikens2020pilot]. On the other hand, propensity score matching optimizes directly for matches which are nearby in terms of the variation associated with the treatment (the "assignment" axis), but it is entirely agnostic to variation associated with the outcome.  This can result in high variance in estimated treatment effect [@king2016propensity].  Finally, the two caliper methods impose contraints on the matching option to ensure that matches are close in terms of propensity score (C) or both propensity and prognostic score (D).  This visualization illustrates the potential of these methods for minimizing bias (stemming from poor propensity score balance) and variace (stemming from poor prognostic score balance) in the treatment effect.

## Assignment-Control Plots and Unmeasured Confounding

While there is a wide and increasing variety of methods which use some articulation of propensity and prognosis to estimate a treatment effect. However, the theorems underlying the use of the propensity score and the theorems underlying the use of the prognostic score both depend on the absense of unmeasured confounding. Violations of this assumption have ramifications for a wide variety of causal inference approaches, and assignment-aontrol plots may be similarly misleading when unmeasured confounding is at play.

Figure 3 illustrates the behavior of assignment control plots in a scenario with unobserved confounding.  We add to our data-generating set-up an unobserved confounder, $U$, such that:

\begin{align*}
    \phi(X_i) &= X_{i1}/c_1 -  \eta U - c_0,\\
    \Psi(X_i) &=\rho X_{i1} + \sqrt{(1-\rho^2)}X_{i2} + \eta U.
\end{align*}

Suppose we somehow ascertained exactly the correct relationships between the two score models and the *observed* covariates, so that our propensity and prognostic models are precisely $\tilde{\phi}(X_i) = X_{i1}/c_1 - c_0$ and $\tilde{\Psi}(X_i) = \rho X_{i1} + \sqrt{(1-\rho^2)}X_{i2}$, respectively.  That is, the score models are exactly correct, except that they do not include the unobserved confounder.  Figure 3 panels A-C depict the assignment control plots we might make and matchings we might produce using these score models, $\tilde{\phi}$ and $\tilde{\Psi}$.  Since both the assignment-control plots and the matchings use the score models $\tilde{\phi}$ and $\tilde{\Psi}$ that fail to capture unobserved confounding, our propensity matches appear quite close in $\tilde{\phi}$ (Figure 3B) and our mahalanobis distance matchings with propensity and prognostic score calipers appear quite close in the assignment-control space defined by $\tilde{\phi} \times \tilde{\Psi}$ (Figure 3C).  

However, panels D-F in Figure 3 show the same matches in the *true* assignment-control space, in which $\phi$ and $\Psi$ are known to depend on the unobserved confounder, $U$.  In each matching, pairs tend to differ from each other due to baseline variations in the unobserved confounder which were not accounted for in the matching process.  The contrast between Figures 3C and 3F most cleanly illustrate how failing to account for $U$ results in systematic error: in the true assignment-control space, one matched individual in each pair tends to have both higher prognostic score and higher propensity score than its partner. Since this individual is more often the treated individual than the control individual, estimates of treatment effect based on this matching will tend to be biased, since $U$ induces systematic differences between paired individuals, even after matching.


```{r}
AC_plot_xSITA <- function(data, rho, nu = 0.2, title = ""){
  plt_data <- data %>%
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2 + nu * U,
           prop = mu,
           t = as.factor(abs(1-t)),
           a = ifelse(t == 1, 0.9, 1)) %>%
    dplyr::select(c(t, prog, prop, a))
  
  plt <- ggplot(data = plt_data, aes( x = prop, y = prog, group = t, color = t)) + 
    geom_point(size = 1, aes(alpha = a)) +
    scale_color_brewer(palette="Set1") +
    ggtitle(title) +
    theme(legend.position = "none", aspect.ratio=1, plot.title = element_text(hjust = 0.5, size = 9))+
    ylab(expression(paste("Prognosis, ", Psi, "(x)", sep = ""))) +
    xlab(expression(paste("Propensity, ", phi, "(x)", sep = "")))
  
  return(plt)
}
```

```{r}
match_viz_xSITA <- function(data, match, rho, nu, k = 1, title = "Matching"){
  plt_data <- data %>% 
    mutate(m = match) %>%
    mutate(a = ifelse (is.na(m), 0.9, 1)) %>% 
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2 + nu * U, 
           prop = mu,
           t = as.factor(abs(1-t))) %>%
    dplyr::select(c(t, prog, prop, m, a))
  
  m_data <- plt_data %>% 
    filter(!is.na(m)) %>%
    arrange(m, desc(t)) %>% 
    mutate(id = rep(1:(k + 1), sum(data$t))) %>%
    dplyr::select(-c(t, a)) %>%
    group_by(m) %>%
    summarize(prop1 = first(prop), prop2 = last(prop),
              prog1 = first(prog), prog2 = last(prog)) %>%
    dplyr::select(prog1, prog2, prop1, prop2)
  
  plt <- ggplot(data = plt_data, aes( x = prop, y = prog, group = t, color = t)) + 
    geom_point(aes(alpha = a), size = 1)+
    scale_color_brewer(palette="Set1") +
    geom_segment(data = m_data, 
                 aes(x = prop1, y = prog1,
                     xend = prop2, yend = prog2),
                 color =  "black", group = NA, linetype = "dashed") +
    ggtitle( title)+
    theme(legend.position = "none", aspect.ratio=1, plot.title = element_text(hjust = 0.5, size = 9))+
    ylab(expression(paste("Prognosis, ", Psi, "(x)", sep = ""))) +
    xlab(expression(paste("Propensity, ", phi, "(x)", sep = "")))
  
  return(plt)
}
```

```{r}
nu <- 0.3
rho <- 0.2
#simulate data
df <- generate_xSITA_data(N = 2000, p = 10, nu = nu, rho = rho, sigma = 1)
k = 1
prop_model = formula(t ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10)
prog_model = formula(y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10)

# mahalanobis match
mahal_dist <- match_on(prop_model, method = "mahalanobis", data = df)
m_match <- pairmatch(mahal_dist, controls = k, df)
```

```{r}
#Calculate best possible propensity and prognostic scores, and match on those
oracle_df <- df %>% 
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2, 
           prop = 1/(1+exp(-(mu - nu * U))))

oracle_prop_match <- pairmatch(t ~ prop, controls = k, oracle_df)
mahal_caliper_dist <- addcaliper(mahal_dist, z = oracle_df$t, p = oracle_df$prop, caliper = 0.1)
mahal_caliper_dist <- addcaliper(mahal_caliper_dist, z = oracle_df$t, p = oracle_df$prog, caliper = 0.1)
m_caliper_match <- pairmatch(mahal_caliper_dist, controls = k, df)
```

```{r,  fig.width=8, fig.height= 6}
naive_df <- df %>%
  mutate(mu = mu - nu * U)

a <- match_viz_xSITA(naive_df, m_match, rho, nu = 0, title = "Mahalanobis Match")
b <- match_viz_xSITA(naive_df, oracle_prop_match, rho, nu = 0, title = "Propensity Match")
c <- match_viz_xSITA(naive_df, m_caliper_match, rho, nu = 0, title = "Mahalanobis Match, Joint Calipers")

d <- match_viz_xSITA(df, m_match, rho, nu, title = "Mahalanobis Match")
e <- match_viz_xSITA(df, oracle_prop_match, rho, nu, title = "Propensity Match")
f <- match_viz_xSITA(df, m_caliper_match, rho, nu, title = "Mahalanobis Match, Joint Calipers")

ggarrange(a,b,c,d, e, f,  ncol= 3, nrow = 2, labels = "AUTO" )
```

## Randomization-Assignment-Control Plots
## Control plots with a Forcing Variable
## Estimating the Score Models

This visualization doesn't actually make sense.  To make this better I would have to empirically fit a propensity and prognostic model, do the three matches and visualize them in the estimated score space, and then compare that to the true score space.

```{r}
# like prognostic match except returns data frame and match assignments, not just the
# reformatted dataframe of outcomes by match assignment
caliper_match_assignment <- function(df, propensity, match_assignment, prog_model, n_control) {
  df$m <- match_assignment
  df$row <- 1:nrow(df)
  n_t<- sum(df$t)

  selected <- df %>% 
    filter(!is.na(m)) %>%
    filter(t==0) %>%
    group_by(m) %>%
    sample_n(size = 1)
  
  prognostic <- lm(prog_model, data = selected)
  not_selected <- df[-selected$row, ]
  not_selected <- not_selected %>% 
			mutate(progscore = predict(prognostic, not_selected)) %>%
			mutate(propscore = predict(propensity, not_selected))
  
  mahal_dist <- match_on(prop_model, method = "mahalanobis", data = not_selected)
  mahal_caliper_dist <- addcaliper(mahal_dist, z = not_selected$t, p = not_selected$propscore, caliper = 0.1)
  m_caliper_match <- pairmatch(mahal_caliper_dist, controls = k, not_selected) 
  return(list(df = not_selected, match = m_caliper_match, k = n_control))
}
```

```{r}
# Build scores empirically for propensity and prognostic match

# build propensity score
propensity <- glm(prop_model, family = binomial(), data = df)
  
prop_match <- pairmatch(propensity, controls = k, df)
  
# 1:2 mahalanobis matching to select data to use for prognostic model
mahal_match <- pairmatch(mahal_dist, controls = 2, df) 
  
caliper_match_assignment <- caliper_match_assignment(df, propensity, mahal_match, prog_model, k)
```

```{r, fig.width=8.2, fig.height= 3, eval = F}
a <- match_viz(df, m_match, rho, title = "Mahalanobis Match")
b <- match_viz(df, prop_match, rho, title = "Estimated Propensity Match")
c <- match_viz(buff_match_assignment$df, buff_match_assignment$match, rho, title = "Estimated Propensity x Prognosis Match")

ggarrange(a,b,c, ncol= 3, labels = "AUTO" )
```

# Conclusion

A modern shift towards an emphasis on large, passively collected datasets presents a host of challenges and opportunities for researchers interested in causality.  As we collect wider datasets with more measured covariates, it will be increasingly important to prioritize the baseline variation that is most important to the causal question - leveraging measured covariates which are useful, and ignoring measured covariates which are uninformative. The complementary tools of the propensity and prognostic scores are well-suited to aid in this endeavor because they summarize two important aspects of baseline variation in the measured covariates: variation associated with the assignment mechanism, and variation associated with the potential outcomes.  Assignment-control plots and variations thereof can be thought of as dimensionality reduction tools in that they digest a possibly very large covariate space into a meaningful reduced space that is easier to use and understand. 



# References
